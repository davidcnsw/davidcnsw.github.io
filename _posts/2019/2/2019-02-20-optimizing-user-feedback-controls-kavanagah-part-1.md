---
title: "Optimizing documentation feedback forms -- Part I"
categories:
- academics-and-practitioners
- user-centered-documentation
keywords:
bitlink: http://bit.ly/
description: "User feedback is a common feature of online help these days. They are a quick and easy method of surveying users' experience of documentation after deployment. However, the feedback feature can sometimes appear to have been added as an afterthought, so I asked the question: how do they measure up to principles of design and content? In this essay, I analysed the feedback methods applied in the online help systems of twenty project management products"
published: false
---

Donal Kavanagh is a technical writer of several years' experience in the software industry. He is pursuing a part MA in Technical Communication and E-Learning with the University of Limerick. This research essay was written as part of that programme [https://www.ul.ie/graduateschool/course/technical-communication-e-learning-ma].

## Measuring Principles of Information Design and Content Writing in Online Help Feedback Features

by Donal Kavanagh, MA in Technical Communication and E-Learning

## Abstract

The option to provide feedback is now a common feature in online help systems. Research into user feedback however, is largely concerned with the pre-deployment stage. This report analyses the information design and content of feedback features in 20 online help systems &mdash; each of which supports users of a project management software tool.

Quantitative and qualitative research is undertaken. The categorisation and counting of design and content elements yields the quantitative data. Qualitative data is generated by a discussion of the sample texts that is underpinned by relevant research and principles of instructional design and content writing.

The report finds that feedback formats are, for the most part, designed in a manner that successfully contrast with the surrounding instructional content and attracts the user's attention. It also finds that a lack of acknowledgement of user feedback, combined with a dearth of information about how and if the feedback will be used, does not motivate the user to continue providing feedback. In addition, a poor standard of content exists that is unlikely to elicit a focused response from the user or generate quantifiable data for the authors of the online help systems.

## 1. Introduction

The option to provide feedback is now a common feature in online help systems. The purpose of this report is to describe a quantitative and qualitative analysis I undertook to examine whether principles of information design and content writing are applied in the feedback features of online help systems.  

The scope of the report is a sample set of 20 online help systems with feedback features. The systems all support users of project management software tools. The quantity and quality of feedback attained through these feedback models is not within the scope of this report.

The following is the layout of the report:

*   Section 1 introduces the report topic and method and outlines the scope and limitations of the research.
*   Section 2 reviews the research and textbook literature in the following areas: usability of online help, technological advances and online help, post-deployment feedback in software, online help and user frustration, and design and writing considerations in terms of feedback gathering.
*   Section 3 describes the methodology by which the twenty sample texts were selected and analysed.
*   Section 4 explains how to interpret the tables of results and, with the aid of screen shots, the categories that I applied to the sample texts.
*   Section 5 presents the results of the information design analysis, summarises the results and discusses the data in terms of the research finding and principles outlined in Section 2.
*   Section 6 presents the results of the content analysis, summarises the results and discusses the data in terms of the research finding and principles outlined in Section 2.
*   Section 5 offers conclusions arising from the report findings and recommends areas for further research.


## 2. Literature Review

This section explores past research in the areas of usability and technical developments in online help, post-deployment feedback by software users, user frustrations with online help, and the design and writing considerations in the formulation of a user feedback feature.



### 2.1. Usability of Online Help

Technical communication textbooks and research tend to focus on the usability evaluation of user assistance in the pre-deployment stage &mdash; exploring tasks such as audience analysis, usability testing and heuristic analysis.  

Markel (2012), for instance, outlines tasks such as audience profiling and user observation. With a similar focus on the pre-deployment stage, Kanter _et al_ (2002) applies best practice from software development, including Nielsen's ten heuristics, to create a process for evaluating the usability of online help with the aid of a structured heuristic task list.  

While writings and research on pre-deployment usability can inform the design of a feedback feature in an online help system &mdash; by applying the usability heuristic of recognition to a feedback option, for example &mdash; they do not examine the value of post-deployment user feedback, and how it is employed to improve the usability of online help. Aided by technological developments, a growing number of online help systems solicit feedback from users.



### 2.2. Technological Advances and Online Help

By increasing the number of channels by which information and feedback is received, Web 2.0 technologies have made technical communication an increasingly collaborative activity (Kunz 2010). User feedback is readily available at every stage in the content lifecycle.

In addition, agile development and the topic-based approach to online help break documentation into dynamic chunks that facilitate granular feedback by users and a quick review and editing process (Andersen 2014; Kunz 2010). Andersen states that companies such as IBM, Microsoft and Adobe Systems are leading the way with the collaborative approach to documentation.

Scott Abel (2011) goes as far as to herald a new era of online help, what he terms Help 2.0. This recent iteration harnesses user communities, co-authoring and search engine optimization to acquire user knowledge. Technical writers are encouraged to (p.3) "provide your customers with an easy way to provide feedback. Errors, omissions, and confusing content can be pointed out by your customers, or by your support staff, so that improvements can be made quickly."

"Feedback loops" (Gurak and Hocks 2009, p.37) are stages in the documentation process when feedback is gathered from users and the documentation is revised and improved accordingly. The authors emphasise the primacy of feedback solicited early in the process but concede that post-publication feedback is worthwhile as it can be "used when the document is modified in the future".  The overall effect is to downplay post-deployment feedback, but as noted by Abel and others, the trend in online help is for a quick turnaround and republication of revised content. Technical writers working with online help for SaaS (Software as a Service) products, which are deployed instantaneously, will be especially cognizant of the potential of this strategy.

Technically therefore, post-deployment feedback is now readily attainable. But what are the principles of effectively capturing user feedback in published software, and what are the challenges that arise? Research in this area is explored next.



### 2.3. Post-Deployment Feedback in Software

Nichols _et al_ (2003) review the available research on post-deployment software usability. By seeing general software users as users of online help, lessons can be drawn by technical communicators. The authors find that (p.4) "the value of collecting usability data from real users as part of their day-to-day activities has been clearly recognized".  Nichols _et al_ identify design principles of issue reporting, including users' comments, which might be applicable to a topic-feedback feature. These include ubiquity, registration and anonymity, and ease of use.  

In proposing a model of continuous feedback by software users, Maalej _et al_ (2009) identify a relevant challenge that arises when the user is actively and explicitly sending feedback, as opposed to when the information is actively sought by the software provider through workshops for example, or implicit feedback is gathered through analysis of software usage. The authors state that (p.983) "an explicit feed-back in a push communication requires considerable effort and motivation from users. In order to be useful, user in-put must be complemented by meaningful comments. Given users' main concern is to accomplish their tasks and not to give feedback, they might either provide a low-quality input or just avoid the submission".

Before the technical writer and their employer commits to acquiring post-deployment user feedback, assurance is needed that a demand exists: that users are frustrated by their experience of online help and their frustrations can be resolved through feedback.


### 2.4. Online Help and User Frustration

Novick and Ward's (2006) research involves in-depth interviews of 25 workplace users of printed and online user assistance. Although the sample is small, the users are non-technical professionals and therefore representative of the wider population that refers to user assistance. Interviews are conducted around five themes: navigation, appropriateness of explanations, problem-oriented organization, presentation, and completeness and correctness. Regarding appropriateness of explanations, over half of the interviewees report that the documentation was either overly-technical or too simple. Nearly half of interviewees highlight the importance of completeness and correctness in online documentation.  

These findings are relevant to this report as they are issues that are particularly suitable for granular feedback on a topic-by-topic basis. In their conclusion, Novick and Ward envisage an online help system that facilitates quantifiable feedback options where the user can select a response such as (p.8) "too basic, give me more detail", or "too advanced, give me the basics".

The research cited so far confirms the value of user feedback and provides some indication of how the feedback option might be presented to the user of online help, but a gap exists in terms of analysing the effectiveness of different design approaches.


### 2.5. Design Considerations

Aside from the design considerations suggested by the Nichols _et al_ issue reporting model, others can be gleaned from web design principles such Brinck _et al_'s (2002, pp.411-415) ten web guidelines. For example:

*   Navigation &mdash; is the feedback option easily found?
*   Simplicity &mdash; is the feature easily understood and clearly presented?
*   Consistency and Contrast &mdash; does the feedback contrast sufficiently with the surrounding instructional content?

Kimball and Hawkins (2008) discuss core principles of design and their application to documentation. In terms of constructivism, the authors highlight the importance of (p.49) "clear visual cues about the structure and content of documents. For example, obvious headings, labels, and titles for text and graphics..." A feedback feature therefore, should be easy to discern from the surrounding instructional content.  

Regarding ecological perception and the concept of affordances, Kimball and Hawkins urge document designers to (p.50) "provide affordances in documents so users understand what they can use to work with the document". For online documentation, they recommend "clear and definite navigational cues, such as links, buttons, and form fields that users can easily recognize as design objects they can do something with".  A strong inference might be drawn by designers of online help: a feedback button or link is not something that users might expect to see, unlike a search function for example, and should therefore be designed to catch their attention.

Visual rhetoric, also explored by Kimball and Hawkins, extends to the use of document design to persuade users.  The authors apply (pp.64-65) Aristotle's three principles of rhetoric &mdash; ethos, pathos and logos &mdash; to explain how this goal is achieved. In the context of user feedback, the principles might be interpreted as follows:

*   Ethos represents the character of the organisation - does the design indicate that the users' feedback matters to the organisation?
*   Pathos refers to the "emotional response in the user" - does the design entice the user to submit feedback?
*   Logos is the "logical or factual information conveyed" - is the feedback process clearly explained to the user?

As indicated by the Maalej _et al_'s  model of continuous feedback, convincing the user to submit feedback is a challenge. The burden is not the designer's alone however, an effective writing style also plays a part.

### 2.6. Writing Considerations

To determine the purpose of their writing, Markel (2012, p.109) encourages writers to ask themselves the following: "When your readers have finished reading what you have written, what do you want them to know or believe? What do you want them to do? Your writing should help readers understand a concept, hold a particular belief, or carry out a task." It follows that on reading the text that is part of a feedback section, users ought to be able to do the following:

*   Understand the feedback process.
*   Believe that the feedback will be acted upon.
*   Successfully submit feedback that accurately reflects their experience.

Where feedback involves the user answering set questions, as opposed to entering free text comments for example, best practice in question writing is also a consideration. Hackos (1994, p.538) examines the area of writing effective questions for customer-satisfaction surveys. Questions "should be carefully designed to ensure that the results are meaningful" and "unambiguous". The author also recommends that questions "should be designed to give you information you can use as a baseline of customer satisfaction".

### 2.7. Summary

The research indicates that feedback acquired through online help is possible and beneficial to user assistance. The challenge of capturing users' attention and motivating them to engage informs technical communicators of how they might present an effective feedback function. However, a research gap exists in terms of existing online help systems that support user feedback. How do they do it and how do they measure up against proven principles of design, usability and technical writing?

In the following section I explain the methodology by which I sourced sample online help publications and analysed the design and writing strategies applied in their user feedback features.

## 3. Methodology

I analysed the information design and content of feedback features provided in 20 online help systems. My research was conducted with the aim of testing an initial hypothesis and answering the attendant question.

**Hypothesis**: _Best practice principles of information design and content writing are not applied to user feedback options of online help systems for project management software tools._

**Question**: _How visible are principles of information design and content writing in the user feedback options of online help systems for project management software tools?_

### 3.1. Data Gathering

For sample texts I gathered the feedback options of online help systems for project management software products. I considered gathering sample texts from the online help of different types of software products, which would have lent greater variance to the results, but this approach would not have been systematic. By specifying a single type of product, I ensured that the findings emanate from a stable body of data. The sample texts address the same types of users who share the same level of motivation to provide feedback. This is an important consideration as motivating users to respond is a significant challenge when creating a software feedback feature (Maalej _et al_ 2009). In addition, project management software is a horizontal market &mdash; its users come from all industries and sectors. In this way the data set is representative of online help systems in the wider landscape of commercial software products, as the sample texts address a very wide business audience.  

I selected the sample texts by using the Capterra.com software review website. It lists project management software products according to various criteria. Beginning from the top of the screen sorted by the Highest Rated products (Capterra 2018), I systematically visited the online help systems of the products listed. I added the feedback feature of each help system to my list of sample texts until data saturation was achieved &mdash; that is, repetitive patterns of design and content emerged, and different approaches to user feedback ceased to be revealed.  For objectivity, I recorded the project management products that are omitted because I could not find or access an online help system, the online help does not provide a feedback feature, or the system is otherwise unsuitable for inclusion. Examples of the latter include where the help is provided in a blog rather than an online help system, or where the topic is accompanied by a comments forum that does not refer to topic feedback and where support issues are raised and addressed. See the Appendix for a full list of the texts that were considered and the codified names of the 20 products that were included in the research.

Texts with an accessible user feedback feature that is suitable for the purposes of this study were systematically added to my data set as I encountered them.  In this way, I ensured that my data gathering was unbiased and systematic.

### 3.2. Data Analysis

I evaluated the sample texts on a quantitative and qualitative basis. I organised the quantitative data into two codified tables, one for information design and one for content. These provided empirical evidence of the application of different design and content elements. For example, the information design table tallied the instances that the user can rate the help topic that they are providing feedback on, and the content table tallied the instances when follow-up questions (such as 'Can you please tell us how we can improve this article?') are employed. I grouped the elements that were counted into categories and summarised the findings to provide a thematic overview. To further aid the reader's understanding of the data, I also assembled screen shots of the design and content elements that were quantified.  

In addition, I qualitatively assessed the data by interpreting the meanings behind the empirical findings, a process whereby I discussed the data using the theories and best practice guidelines outlined in the Literature Review chapter.  

In the following chapter, Results & Discussion, the results are presented and the findings summarised and discussed by category.

## Next section

Continue on to [Optimizing documentation feedback forms -- Part II](/optimizing-user-feedback-controls-kavanagah-part-2/).
